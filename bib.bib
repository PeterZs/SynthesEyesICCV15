@inproceedings{schneider2014manifold,
  title={Manifold Alignment for Person Independent Appearance-based Gaze Estimation},
  author={Schneider, Timo and Schauerte, Boris and Stiefelhagen, Rainer},
  booktitle={Proc. ICPR},
  pages={1167--1172},
  year={2014},
  organization={IEEE}
}

@inproceedings{funes2013person,
  title={Person independent 3d gaze estimation from remote {RGB-D} cameras},
  author={Funes Mora, Kenneth Alberto and Odobez, Jean-Marc},
  booktitle={Proc. ICIP},
  year={2013},
  organization={IEEE}
}

@inproceedings{kaneva2011evaluation,
  author={Kaneva, B. and Torralba, A. and Freeman, W.T.},
  booktitle={ICCV},
  title={Evaluation of image features using a photorealistic virtual world},
  year={2011},
  pages={2282-2289},
}

@article{zface,
  title={{Dense 3D Face Alignment from 2D Videos in Real-Time}},
  author={Jeni, L{\'a}szl{\'o} A and Cohn, Jeffrey F and Kanade, Takeo},
  journal={FG},
  publisher = {IEEE Computer Society},
  year={2015}
}

@inproceedings{karthikeyan2013and,
  title={From where and how to what we see},
  author={Karthikeyan, S and Jagadeesh, Vignesh and Shenoy, Renuka and Ecksteinz, Miguel and Manjunath, BS},
  booktitle={ICCV},
  pages={625--632},
  year={2013},
}

@inproceedings{sattar15_cvpr,
title = {Prediction of Search Targets From Fixations in Open-World Settings},
author = {Hosnieh Sattar and Sabine M\"uller and Mario Fritz and Andreas Bulling},
year = {2015},
booktitle = {Proc. CVPR},
abstract = {Previous work on predicting the target of visual search from human fixations only considered closed-world settings in which training labels are available and predictions are performed for a known set of potential targets. In this work we go beyond the state-of-the-art by studying search target prediction in an open-world setting. To this end, we present a dataset containing fixation data of 18 users searching for natural images from three image categories within image collages of about 80 images. In a closed-world baseline experiment we show that we can predict the correct mental image out of a candidate set of five images. In an open-world experiment we no longer assume potential search targets to be part of the training set and we also no longer assume that we have fixation data for these targets. We present a new problem formulation for search target recognition in the open-world setting, which is based on learning compatibilities between fixations and potential targets.},
}

@inproceedings{yun2013studying,
    Author = {Yun, Kiwon and Peng, Yifan and Samaras, Dimitris and Zelinsky, Gregory J and Berg, Tamara L},
    Booktitle = {CVPR},
    Pages = {739--746},
    Title = {Studying relationships between human gaze, description, and computer vision},
    Year = {2013}}

@incollection{papadopoulos2014training,
    Author = {Papadopoulos, Dim P and Clarke, Alasdair DF and Keller, Frank and Ferrari, Vittorio},
    Booktitle = {ECCV},
    Pages = {361--376},
    Title = {Training Object Class Detectors from Eye Tracking Data},
    Year = {2014}}

@inproceedings{bulling13_chi,
title = {{EyeContext: Recognition of High-level Contextual Cues from Human Visual Behaviour}},
author = {Andreas Bulling and Christian Weichel and Hans Gellersen},
isbn = {978-1-4503-1899-0},
year = {2013},
date = {2013-04-27},
booktitle = {CHI},
pages = {305-308},
abstract = {In this work we present EyeContext, a system to infer high-level contextual cues from human visual behaviour. We conducted a user study to record eye movements of four participants over a full day of their daily life, totalling 42.5 hours of eye movement data. Participants were asked to self-annotate four non-mutually exclusive cues: social (interacting with somebody vs. no interaction), cognitive (concentrated work vs. leisure), physical (physically active vs. not active), and spatial (inside vs. outside a building). We evaluate a proof-of-concept EyeContext system that combines encoding of eye movements into strings and a spectrum string kernel support vector machine (SVM) classifier. Our results demonstrate the large information content available in long-term human visual behaviour and opens up new venues for research on eye-based behavioural monitoring and life logging.},
}

@article{bulling11_pami,
title = {Eye Movement Analysis for Activity Recognition Using Electrooculography},
author = {Andreas Bulling and Jamie A. Ward and Hans Gellersen and Gerhard Tr\"oster},
year = {2011},
journal = {IEEE TPAMI},
abstract = {In this work we investigate eye movement analysis as a new sensing modality for activity recognition. Eye movement data was recorded using an electrooculography (EOG) system. We first describe and evaluate algorithms for detecting three eye movement characteristics from EOG signals - saccades, fixations, and blinks - and propose a method for assessing repetitive patterns of eye movements. We then devise 90 different features based on these characteristics and select a subset of them using minimum redundancy maximum relevance feature selection (mRMR). We validate the method using an eight participant study in an office environment using an example set of five activity classes: copying a text, reading a printed paper, taking hand-written notes, watching a video, and browsing the web. We also include periods with no specific activity (the NULL class). Using a support vector machine (SVM) classifier and a person-independent (leave-one-out) training scheme, we obtain an average precision of 76.1% and recall of 70.5% over all classes and participants. The work demonstrates the promise of eye-based activity recognition (EAR) and opens up discussion on the wider applicability of EAR to other activities that are difficult, or even impossible, to detect using common sensing modalities.},
}

@inbook{majaranta14_apc,
title = {Eye Tracking and Eye-Based Human-Computer Interaction},
author = {Päivi Majaranta and Andreas Bulling},
editor = {Stephen H. Fairclough and Kiel Gilleade},
isbn = {978-1-4471-6391-6},
year = {2014},
date = {2014-04-01},
publisher = {Springer},
series = {Advances in Physiological Computing},
abstract = {Eye tracking has a long history in medical and psychological research as a tool for recording and studying human visual behavior. Real-time gaze-based text entry can also be a powerful means of communication and control for people with physical disa-bilities. Following recent technological advances and the advent of affordable eye trackers, there is a growing interest in pervasive at-tention-aware systems and interfaces that have the potential to rev-olutionize mainstream human-technology interaction. In this chapter, we provide an introduction to the state-of-the art in eye tracking technology and gaze estimation. We discuss challenges involved in using a perceptual organ, the eye, as an input modality. Examples of real life applications are reviewed, together with design solutions derived from research results. We also discuss how to match the user requirements and key features of different eye tracking sys-tems to find the best system for each task and application.},
}

@article{gross2010multi,
  title={Multi-pie},
  author={Gross, Ralph and Matthews, Iain and Cohn, Jeffrey and Kanade, Takeo and Baker, Simon},
  journal={Image and Vision Computing},
  year={2010},
  publisher={Elsevier}
}

@inproceedings{sagonas2013300,
  title={300 faces in-the-wild challenge: The first facial landmark localization challenge},
  author={Sagonas, Christos and Tzimiropoulos, Georgios and Zafeiriou, Stefanos and Pantic, Maja},
  booktitle={ICCVW},
  pages={397--403},
  year={2013},
}

@inproceedings{liebelt2010multiview,
  author={Liebelt, J. and Schmid, C.},
  booktitle={CVPR},
  title={Multi-view object class detection with a 3D geometric model},
  year={2010},
  pages={1688-1695},
}

@article{jaderberg2014synthetic,
  title={Synthetic data and artificial neural networks for natural scene text recognition},
  author={Jaderberg, Max and Simonyan, Karen and Vedaldi, Andrea and Zisserman, Andrew},
  journal={arXiv preprint},
  year={2014}
}

@inproceedings{yu2010improving,
  title={Improving person detection using synthetic training data},
  author={Yu, Jie and Farin, Dirk and Krüger, Christof and Schiele, Bernt},
  booktitle={ICIP},
  year={2010}
}

@inproceedings{baltrusaitis20123d,
  title={3D constrained local model for rigid and non-rigid facial tracking},
  author={Baltrusaitis, Tadas and Robinson, Peter and Morency, L},
  booktitle={Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on},
  pages={2610--2617},
  year={2012},
  organization={IEEE}
}

@inproceedings{xiong2014gaze,
  title={Gaze Estimation Based on 3D Face Structure and Pupil Centers},
  author={Xiong, Chunshui and Huang, Lei and Liu, Changping},
  booktitle={ICPR},
  year={2014},
}

@inproceedings{Xiong2013sdm,
 title={Supervised Descent Method and its Applications to Face Alignment},
 booktitle={CVPR},
 author={Xiong, Xuehan and De la Torre, Fernando},
 year={2013}
 }
 
@inproceedings{Asthana2013drmf,
 title={Robust Discriminative Response Map Fitting with Constrained Local Models},
 booktitle={CVPR},
 author={Asthana, Akshay and Zafeiriou, Stefanos and Cheng, Shiyang and Pantic, Maja},
 year={2013}
 } 
 
@inproceedings{Zhu2012tree,
 title={Face detection, pose estimation, and landmark localization in the wild},
  ISBN={978-1-4673-1228-8},
   booktitle={CVPR},
  author={Zhu, Xiangxin and Ramanan, Deva}, year={2012}}
 
 
@inproceedings{okada2008relevant,
  title={Relevant feature selection for human pose estimation and localization in cluttered images},
  author={Okada, Ryuzo and Soatto, Stefano},
  booktitle={ECCV},
  year={2008}
}

@inproceedings{shakhnarovich2003fast,
  title={Fast pose estimation with parameter-sensitive hashing},
  author={Shakhnarovich, Gregory and Viola, Paul and Darrell, Trevor},
  booktitle={ICCV},
  year={2003}
}

@inproceedings{orvalho2012facial,
  title={A facial rigging survey},
  author={Orvalho, Ver{\'o}nica and Bastos, Pedro and Parke, Frederic and Oliveira, Bruno and Alvarez, Xenxo},
  booktitle={Eurographics},
  pages={10--32},
  year={2012}
}

@article{feng1998variance,
  title={Variance projection function and its application to eye detection for human face recognition},
  author={Feng, Guo-Can and Yuen, Pong Chi},
  journal={Pattern Recognition Letters},
  volume={19},
  number={9},
  pages={899--906},
  year={1998},
}

@inproceedings{Zhang14_AVIb,
title = {Pupil-canthi-ratio: a calibration-free method for tracking horizontal gaze direction},
author = {Yanxia Zhang and Andreas Bulling and Hans Gellersen},
isbn = { 978-1-4503-2775-6},
year = {2014},
date = {2014-05-27},
booktitle = {AVI},
pages = {129-132},
abstract = {Eye tracking is compelling for hands-free interaction with pervasive displays. However, most existing eye tracking systems require specialised hardware and explicit calibrations of equipment and individual users, which inhibit their widespread adoption. In this work, we present a light-weight and calibration-free gaze estimation method that leverages only an off-the-shelf camera to track users\' gaze horizontally. We introduce pupil-canthi-ratio (PCR), a novel measure for estimating gaze directions. By using the displacement vector between the inner eye corner and the pupil centre of an eye, PCR is calculated as the ratio of the displacement vectors from both eyes. We establish a mapping between PCR to gaze direction by Gaussian process regression, which inherently infers averted horizontal gaze directions of users. We present a study to identify the characteristics of PCR. The results show that PCR achieved an average accuracy of 3.9 degrees across different people. Finally, we show examples of real-time applications of PCR that allow users to interact with a display by moving only their eyes.},
}

@article{Argyle1965,
author = {Argyle, Michael and Dean, Jean},
journal = {Sociometry},
title = {{Eye-Contact, Distance and Affiliation.}},
year = {1965}
}

@inproceedings{shotton2013real,
  title={Real-time human pose recognition in parts from a single depth image},
  author={Shotton, Jamie and Sharp, Toby and Kipman, Alex and Fitzgibbon, Andrew and Finocchio, Mark and Blake, Andrew and Cook, Mat and Moore, Richard},
  booktitle={CVPR},
  year={2011},
}

@inproceedings{zhang14_ubicomp,
title = {GazeHorizon: Enabling Passers-by to Interact with Public Displays by Gaze},
author = { Yanxia Zhang and Hans Jörg Müller and Ming Ki Chong and Andreas Bulling and Hans Gellersen},
isbn = {978-1-4503-2968-2},
year = {2014},
date = {2014-09-13},
booktitle = {UbiComp},
pages = {559-563},
abstract = {Public displays can be made interactive by adding gaze control. However, gaze interfaces do not offer any physical affordance, and require users to move into a tracking range. We present GazeHorizon, a system that provides interactive assistance to enable passers-by to walk up to a display and to navigate content using their eyes only. The system was developed through field studies culminating in a four-day deployment in a public environment. Our results show that novice users can be facilitated to successfully use gaze control by making them aware of the interface at first glance and guiding them interactively into the tracking range.},
}

@inproceedings{baltrusaitis2013constrained,
  title={Constrained local neural fields for robust facial landmark detection in the wild},
  author={Baltru\v{s}aitis, Tadas and Robinson, Peter and Morency, Louis-Philippe},
  booktitle={ICCVW},
  year={2013}
}

@inproceedings{swirski2012robust,
  title={Robust real-time pupil tracking in highly off-axis images},
  author={{\'S}wirski, Lech and Bulling, Andreas and Dodgson, Neil},
  booktitle={ETRA},
  year={2012}
}

@inproceedings{sagar1994virtual,
  title={A virtual environment and model of the eye for surgical simulation},
  author={Sagar, Mark and Bullivant, David and Mallinson, Gordon and Hunter, Peter},
  booktitle={Computer graphics and interactive techniques},
  year={1994}
}

@inproceedings{wood2014eyetab,
  title={Eyetab: Model-based gaze estimation on unmodified tablet computers},
  author={Wood, Erroll and Bulling, Andreas},
  booktitle={ETRA},
  year={2014}
}

@inproceedings{lu2012head,
  title={Head pose-free appearance-based gaze sensing via eye image synthesis},
  author={Lu, Feng and Sugano, Yusuke and Okabe, Takahiro and Sato, Yoichi},
  booktitle={ICPR},
  pages={1008--1011},
  year={2012},
}


@inproceedings{girshick2014rich,
  title={Rich feature hierarchies for accurate object detection and semantic segmentation},
  author={Girshick, Ross and Donahue, Jeff and Darrell, Trevor and Malik, Jitendra},
  booktitle={CVPR},
  pages={580--587},
  year={2014},
}


@inproceedings{zhou2014learning,
  title={Learning deep features for scene recognition using places database},
  author={Zhou, Bolei and Lapedriza, Agata and Xiao, Jianxiong and Torralba, Antonio and Oliva, Aude},
  booktitle={NIPS},
  pages={487--495},
  year={2014}
}


@inproceedings{sugano2014learning,
  title={{Learning-by-Synthesis for Appearance-based 3D Gaze Estimation}},
  author={Sugano, Yusuke and Matsushita, Yasuyuki and Sato, Yoichi},
  booktitle={CVPR},
  year={2014},
}

@inproceedings{zhang15_cvpr,
title = {{Appearance-Based Gaze Estimation in the Wild}},
author = {Xucong Zhang and Yusuke Sugano and Mario Fritz and Andreas Bulling},
year = {2015},
date = {2015-03-02},
booktitle = {CVPR},
abstract = {Appearance-based gaze estimation is believed to work well in real-world settings but existing datasets were collected under controlled laboratory conditions and methods were not evaluated across multiple datasets. In this work we study appearance-based gaze estimation in the wild. We present the MPIIGaze dataset that contains 213,659 images we collected from 15 participants during natural everyday laptop use over more than three months. Our dataset is significantly more variable than existing datasets with respect to appearance and illumination. We also present a method for in-the-wild appearance-based gaze estimation using multimodal convolutional neural networks, which significantly outperforms state-of-the art methods in the most challenging cross-dataset evaluation setting. We present an extensive evaluation of several state-of-the-art image-based gaze estimation algorithm on three current datasets, including our own. This evaluation provides clear insights and allows us identify key research challenges of gaze estimation in the wild.},
keywords = {},
tppubtype = {inproceedings}
}

@inproceedings{bohme2008software,
  title={A software framework for simulating eye trackers},
  author={B{\"o}hme, Martin and Dorr, Michael and Graw, Mathis and Martinetz, Thomas and Barth, Erhardt},
  booktitle={ETRA},
  year={2008},
}

@MANUAL{MIL-STD-1472G,
  title = {MIL-STD-1472G Design Criteria Standard: Human Engineering},
  organization = {Department of Defence, USA},
  month = {January},
  year = {2012}
}

@inproceedings{swirski2014rendering,
  title={Rendering synthetic ground truth images for eye tracker evaluation},
  author={{\'S}wirski, Lech and Dodgson, Neil},
  booktitle={ETRA},
  year={2014}
}

@inproceedings{stratou2011effect,
  title={{Effect of illumination on automatic expression recognition: a novel 3D relightable facial database}},
  author={Stratou, Giota and Ghosh, Abhijeet and Debevec, Paul and Morency, Louis-Philippe},
  booktitle={FG},
  year={2011},
}

@inproceedings{fanelli2011real,
  title={Real time head pose estimation with random regression forests},
  author={Fanelli, Gabriele and Gall, Juergen and Van Gool, Luc},
  booktitle={CVPR},
  year={2011}
}

@INPROCEEDINGS{priamikov14_openeyesim,
author={Priamikov, A. and Triesch, J.},
booktitle={ICDL-Epirob},
title={OpenEyeSim - A platform for biomechanical modeling of oculomotor control},
year={2014},
month={Oct},
pages={394-395},
keywords={control engineering computing;data visualisation;eye;medical computing;medical control systems;solid modelling;virtual reality;OpenEyeSim model;biomechanical modeling;data visualization;eye biomechanical simulation;eye movement;eye muscle control;human extra-ocular eye muscles;oculomotor control;perception-action loop;three-dimensional biomechanical model;virtual environment;visual development;visual representation;Biological system modeling;Biomechanics;Joints;Muscles;Orbits;Pulleys;Visualization},
doi={10.1109/DEVLRN.2014.6983013},}


@inproceedings{alabort2014statistically,
  title={Statistically Learned Deformable Eye Models},
  author={Alabort-i-Medina, Joan and Qu, Bingqing and Zafeiriou, Stefanos},
  booktitle={ECCVW},
  year={2014},
}

@inproceedings{ruhland2014look,
  title={Look me in the eyes: A survey of eye and gaze animation for virtual agents and artificial systems},
  author={Ruhland, Kerstin and Andrist, Sean and Badler, Jeremy and Peters, Christopher and Badler, Norman and Gleicher, Michael and Mutlu, Bilge and Mcdonnell, Rachel},
  booktitle={Eurographics},
  pages={69--91},
  year={2014}
}

@article{debevec2002image,
  title={Image-based lighting},
  author={Debevec, Paul},
  journal={IEEE Computer Graphics and Applications},
  volume={22},
  number={2},
  pages={26--34},
  year={2002},
}

@article{berard2014highquality,
  title={Highquality capture of eyes},
  author={B{\'e}rard, Pascal and Bradley, Derek and Nitti, Maurizio and Beeler, Thabo and Gross, Markus},
  journal={ACM TOG},
  year={2014}
}

@inproceedings{lee2000displaced,
  title={Displaced subdivision surfaces},
  author={Lee, Aaron and Moreton, Henry and Hoppe, Hugues},
  booktitle={SIGGRAPH},
  pages={85--94},
  year={2000},
}

@book{liversedge2011oxford,
  title={The Oxford handbook of eye movements},
  author={Liversedge, Simon and Gilchrist, Iain and Everling, Stefan},
  year={2011},
  publisher={Oxford University Press}
}

howpublished = {\url{http://docs.pixologic.com/user-guide/3d-modeling/topology/zremesher/}}
@misc{ZRemesher,
  title = {ZBrush ZRemesher 2.0 – Automatic retopology taken to a new level},
  year = {2015},
  author = {Pixologic},
}

misc{Shrinkwrap,
  title = {The Shrinkwrap Modifier},
  year = {2015},
  author = {{The Blender Foundation}},
  howpublished = {\url{http://wiki.blender.org/index.php/Doc:2.6/Manual/Modifiers/Deform/Shrinkwrap}}
}

@misc{Cycles,
  title = {Cycles Render Engine},
  year = {2015},
  author = {{The Blender Foundation}},
  howpublished = {\url{http://wiki.blender.org/index.php/Doc:2.6/Manual/Render/Cycles}}
}

@misc{AdaptiveSamplesHDR,
  title = {HDR Panoramas},
  year = {2015},
  author = {Greg Zaal},
  howpublished = {\url{http://adaptivesamples.com/category/hdr-panos/}}
}

@misc{Ten24,
  title = {Face Scanning},
  year = {2015},
  author = {Busby, James and Rawlinson, Chris},
  howpublished = {\url{http://www.ten24.info/?page_id=299}}
}

@misc{ActiBlizEyes,
  title = {Photorealistic Eyes Rendering},
  year = {2012},
  howpublished={SIGGRAPH Advances in Real-Time Rendering},
  author = {Jimenez, Jorge and Danvoye, Etienne and von der Pahlen, Javier},
}