%!TEX root = 00_main.tex

\section{Experiments}

%\commentA{the distribution figures would be nice here, e.g. to make the point that we can synthesise arbitrary distributions and to beef up the gaze estimation experiments}

% \begin{figure}
%     \centering
%     \includegraphics[width=\textwidth]{head_gaze_distribution_v2.pdf}
%     \caption{The head pose (first row) and gaze direction (second row) distribution of different datasets.}
%     \label{fig:head_gaze_distribution}
% \end{figure}

%\commentA{just a reminder to potentially remove the top row in Figure 8 (head pose distributions)}

% \input{figs/fig_example_fits_wild.tex}

% \commentA{briefly say something about significance/importance of both problems, more in corresponding subsections}

\commentY{Maybe this should be presented earlier (in the related works section? we are currently missing references to other appearance-based gaze estimation methods), definition of the two tasks and why and how synthetic data is expected to be helpful. Especially eye-shape registration is not well defined during the previous section}
We evaluated the usefulness of our synthetic data generation method on two sample problems, eye-shape registration and appearance-based gaze estimation.
%
Eye-shape registration attempts to detect anatomical landmarks of the eye -- eyelids, iris and the pupil. 
Such approaches either attempt to model the shape of the eye directly by relying on edge information \cite{wood2014eyetab, swirski2012robust} or by using statistically learnt deformable models \cite{alabort2014statistically}. 
As our method can reliably generate consistent landmark location training data, we use it for Constrained Local Neural Field (CLNF) \cite{baltrusaitis2013constrained} deformable model training.

\commentY{Edited this paragraph}
Appearance-based gaze estimation systems learn a mapping directly from eye image pixels to gaze direction.
While most of the prior work focused on the {\em person-dependent} training scenario which assumes training data from the target user, recently more attention is paid to the {\em person-independent} training scenario~\cite{funes2013person,schneider2014manifold,sugano2014learning,zhang15_cvpr}. 
% This is in contrast to geometry-based approaches that rely on tracking features of 
%This is a challenging task considering the changes in appearance, 
%thus requiring large amounts of training data to perform well.
The training dataset is required to cover the potential changes in appearance with different eye shapes, arbitrary head poses, gaze directions, and illumination conditions.
%
%Since our method allows the synthesis of eye images with arbitrary head poses, gaze directions, and illumination conditions, we can generate a suitable training dataset rapidly, and even tailor it to the target domain.
Compared to \citet{sugano2014learning}, our method can provide a wider range of illumination conditions which can be beneficial to handle the unknown illumination condition in the target domain.


%\subsection{Eye-Shape Registration In the Wild}

\subsection{Eye-Shape Registration}

\commentY{I feel this subsection + paragraph structure is better to summarize two shape registration experiments?}

\paragraph{Eye-Shape Registration In the Wild}

% \commentA{maybe this should also read eye-shape registration in both section heading and text to be consistent with the next subsection}

\begin{figure}
    \centering
    \includegraphics[width=\columnwidth]{figs/CLNF_300W_experiment.pdf}
    \caption{We outperform the state-of-the-art for eyelid-registration in the wild. The right plot shows how performance degrades for training data without important degrees of variation: realistic lighting and eyelid movement.}
    \label{fig:clnf_results_wild}
\end{figure}

% \input{figs/fig_example_fits_wild.tex}

% Erroll to Tadas: need to make sure these facts are straight...
We performed an experiment to see how our our system generalises on unseen and unconstrained images from the 300 Faces In-the-Wild (300-W) challenge \cite{sagonas2013300} validation datasets which contain labels for eyelid boundaries. We tested all of the approaches on the 830 (out of 1026) test images. We discarded images that did not contain visible eyes (occluded by hair or sunglasses) or where face detection failed in some of the baselines. This lead to 1660 eye images for evaluation.

We trained CLNF patch experts using the generic \dataset dataset and used the 3D landmark locations to construct a Point Distribution Model (PDM) using Principal Component Analysis. 
As our rendered images did not contain closed eyes we generated extra closed eye landmark labels by moving the upper eyelid down to lower one or meeting both eyelids halfway.
We initialised our approach by using the face-CLNF \cite{baltrusaitis2013constrained} facial landmark detector.

To compare using synthetic or real training images, we trained an eyelid CLNF model on 300-W images, but used the same PDM used for synthetic data (CLNF 300-W).
We also compared our approach with the following state-of-the-art facial landmark detectors trained on real world in-the-wild data: CLNF \cite{baltrusaitis2013constrained}, Supervised Descent Method (SDM) \cite{Xiong2013sdm}, Discriminative Response Map Fitting (DRMF) \cite{Asthana2013drmf}, and tree based face and landmark detector \cite{Zhu2012tree}. 

The results of our experiments can be seen in \autoref{fig:clnf_results_wild}, and example model fits are shown in \autoref{fig:fits_300W}.
Errors were recorded as the RMS point-to-boundary distance from tracked eyelid landmarks to ground truth eyelid boundary, and were normalized by inter-ocular distance. 
First, the results show the eye-CLNF (both synthetic ($\mathrm{Mdn}=0.0110$) and real data ($\mathrm{Mdn}=0.0110$) outperforming all other systems in eye-lid localization: SDM ($\mathrm{Mdn}=0.0134$), face-CLNF ($\mathrm{Mdn}=0.0139$), DRMF ($\mathrm{Mdn}=0.0238$), and Tree based ($\mathrm{Mdn}=0.0217$). 
Second, our system (CLNF Synth) trained on only ten participants in four lighting conditions results in very similar performance to a system trained on unconstrained in-the-wild images (CLNF 300-W). This suggests the importance of high-quality consistent labels.
% and that our system can be used to achieve state-of-the-art performance for eye shape registration. 
%Furthermore, our approach managed to generalise without explicit modelling of eye-glasses or other partial occlusions.

Our data synthesis system also allows us to examine what steps of the synthesis approach are important for generating good training data. We trained two further eye-CLNFs on different versions of \dataset, one without eyelid motion and one with only one fixed lighting condition. As can be seen in \autoref{fig:clnf_results_wild}, not using shape variation ($\mathrm{Mdn}=0.0129$) and using basic lighting ($\mathrm{Mdn}=0.0120$) leads to worse performance due to missing degrees of variability.

%\subsection{Eye-Shape Registration for Webcams}
\paragraph{Eye-Shape Registration for Webcams}

% \commentA{``for laptops'' sounds very specific. Does this generalise, do we need this strong constraint?}

\input{figs/fig_clnf_fits.tex}

\begin{figure}
    \centering
    \includegraphics[width=\columnwidth]{CLNF_MPII_experiment}
    \caption{We perform comparably with state-of-the-art for iris-registration on in-the-wild webcam images.}
    \label{fig:clnf_results_MPII}
\end{figure}

While the 300-W images represent challenging conditions for eyelid registration, they are not representative of typical webcam-style images and do not feature iris labels.
We therefore annotated sub-pixel eyelid and iris boundaries onto a subset of MPIIGaze (188 images), a recent large-scale dataset of face images and corresponding on-screen gaze locations collected during everyday laptop use over several months \cite{zhang15_cvpr}.
Pupil accuracy was not evaluated as it was impossible to discern in in most images.
We compared our eye-CLNF with EyeTab \cite{wood2014eyetab}, a state-of-the-art shape-based approach for webcam gaze estimation that robustly fits ellipses to the iris boundary using image-aware RANSAC \cite{swirski2012robust}.
We used a modified version of the author's implementation with improved eyelid localization using CLNF \cite{baltrusaitis2013constrained}.
As a baseline, we used the mean position of all 28 eye-landmarks following model initialization.
Eyelid errors were calculated as RMS distances from eyelid landmarks to the ground truth eyelid boudnary.
Iris errors were calculated by first least-squares fitting an ellipse to the tracked iris landmarks, discretizing it, removing points outside ground truth and tracked eyelid boundaries, and then measuring RMS distances to the ground truth iris.
This avoided calculating errors for parts of the iris which were occluded by the eyelid.
Errors were normalized by the eye-width, and are reported using average eye-width ($44.4\textrm{px}$) as reference.

As shown in \autoref{fig:clnf_results_MPII}, our approach ($\textrm{Mdn}\!=\!1.48\textrm{px}$) demonstrates comparable iris-fitting accuracy with EyeTab ($\textrm{Mdn}\!=\!1.44\textrm{px}$), a state-of-the-art algorithm for fitting ellipses to irises in low-quality images.
However, our eye-CLNF is more robust, with EyeTab failing to terminate in $2\%$ of test cases.
As also shown by the 300-W experiment, our eye-CLNF localizes eyelids better than the face-CLNF.
See \autoref{fig:fits_MPII} for example model fits.
%As shown by \todo{prev exp.}, our eye-CLM ($2.79\!\pm\!2.20\textrm{px}$) also localizes eyelids better than previous state-of-the-art facial landmark trackers ($4.27\!\pm\!1.84\textrm{px}$).

% show that we only need data from few(er) people and show competitive performance
% Maybe) Plot landmark accuracy on LFW against number of training participants. Show that even with just a few participants (e.g. 4) we get good results for eyelid positions compared to state-of-the-art face trackers.

% eye corner detection
% eye bounding box detection
% eye position detection?
% ^ I think all of these come with what the deformable model gives us

\subsection{Appearance-Based Gaze Estimation}

% evaluate eye/gaze/eyelid shapes (fully synthetic) separately from full face appearance (which is a mixture of real and synthetic data)

% person-adaptation, use pre-trained model from synthesised data, then personalise with small amount of user-specific data
% ^ let's leave this as future work! Can put it in the discussion 

% show that we can synthesise specific datasets for specific settings (specific head and gaze ranges, illumination conditions), show that we can competitive performance

\begin{figure}
    \centering
    \includegraphics[width=\columnwidth]{head_gaze_distribution_v2.pdf}
    \caption{The gaze direction (first row, blue) and head pose (second row, red) distributions of different datasets.}
    \label{fig:head_gaze_distribution}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=\columnwidth]{gazeResult.pdf}
    \caption{Gaze estimation performance comparison. The X axis indicates the test set is the MPIIGaze dataset or its frontal pose subset. The legend shows different training sets.}
    \label{fig:gazeResult}
\end{figure}

\commentY{Edited this paragraph}
To evaluate the usage of our method on appearance gaze estimation, we perform the cross-dataset validation as described in \citet{zhang15_cvpr}, where they train and test the model on different datasets. 
We synthesized our dataset using the same camera setting as UT dataset~\cite{sugano2014learning}, and the same normalization scheme can be applied to the test data.
The training data is fully compatible with UT dataset, and we can directly compare our~\dataset using the same Convolutional Neural Network (CNN) model~\cite{zhang15_cvpr}.

%Following the same setting, we train the same Convolutional Neural Network (CNN) model on the generic~\dataset dataset, and test it on MPIIGaze evaluation subset proposed by~\cite{zhang15_cvpr}. 
As shown with the two bars at the far left of~\autoref{fig:gazeResult}, compared to the mean gaze direction prediction error 13.9 degrees that trained with UT dataset, the model trained with our generic~\dataset dataset can achieve the same performance with 14.0 degrees mean error. It confirms that our method can generate a equivalent data for the appearance-based gaze estimation training.
\commentY{Moved \& edited}
The fifth bar of~\autoref{fig:gazeResult} shows the leave-one-person-out cross validation within the MPIIGaze dataset to show the up bounder of this scenario.
These result indicates that both \dataset~and UT datasets still have their own error sources which is causing the performance gap.

%\paragraph{Head Pose and Gaze Ranges}
\commentY{Edited}
One of the important factors related to the above gap is the head pose and gaze ranges.
While it is important to cover a wide range of head poses to handle arbitrary camera settings, this can make the training task more difficult.
If the target setting can be preliminary specified, as the laptop interaction case in \citet{zhang15_cvpr}, it is practically possible to limit the synthesis of training data with minimum required head pose and gaze ranges.
%The head pose and gaze direction range are also quite important priors for appearance-based gaze estimation. 
%Since the samples from the small range of head poses can be trained together, while extreme head poses wouldn't share eye appearance. Also the gaze direction space is arbitrary and difficult to be densely covered by collected dataset.
In order to analyze the effect of different head pose ranges, we additionally render a~\dataset subset that matches the gaze and pose distribution of MPIIGaze dataset.
Since our method also allows us to control lighting conditions, we also added 3D laptop screen emitting light to simulate the target condition.
%Notice that the head pose and gaze direction range can be estimated based on the task, so the required information is not sophisticated. 
%This shows how we can target specific scenarios like laptop-based gaze estimation, and render a suitable dataset within a day rather than taking 3 months of data collection with 15 participants. 
For comparison, we also re-sample a subset of UT dataset as described in~\cite{zhang15_cvpr} that has the same gaze and head pose distribution with MPIIGaze.
The head pose and gaze direction distribution are shown in~\autoref{fig:head_gaze_distribution}.

Another important difference between two datasets is the number of subjects in the dataset.
%Since there are just have 10 subjects in our~\dataset, 
In order to compare two approaches with the same number of subjects, we divide the UT Multiview subset into 5 groups with 10 subjects.
Each group of this UT subset has 15,000 samples as with our~\dataset subset.
We then average the performance of the 5 groups for the final result. 
As shown in the third and forth bars of~\autoref{fig:gazeResult}, in general having the similar head pose and gaze direction ranges of target domain can significantly improve the performance. 
\commentY{Put actual numbers here} 
Since both UT subset and~\dataset subset have the similar head pose and gaze direction range, the performance gains should come from the realistic samples generate from precise 3D eye model and also simulated variant appearance caused by light conditions.
This result indicates, under the condition with the same number of face variations, our~\dataset subset can be a better training set compared to UT subset.

\paragraph{Difference Between Synthesis and Realistic Data}
The UT dataset is synthetically generated based on collected data, while our~\dataset is purely synthesis, the different affect between them for the gaze estimation training is interesting. We thus based on the the above experiments to play around their roles in such cross datasets scenario.
We use the pre-trained model on the~\dataset and fine tune it with the UT dataset to test on MPIIGaze dataset, it is the case that we can pre-train the model with the synthetic~\dataset and fine tune the model if we have more realistic data. For caparison, we test the opposite way, i.e. pre-train the model in UT dataset and fine tune it with the~\dataset. As shown in~\autoref{tab:Synthesis_Realistic_Gaze}, our~\dataset is suitable as training data due to its generality while UT doesn't have such quality. 

\begin{table}
\begin{center}
\begin{tabular}{ |c|p{80pt}|p{80pt}| } 
 \hline
 Train set & pre-train~\dataset and fine tune with UT & pre-train UT and fine tune with~\dataset \\ 
 \hline
 Full & X.X & X.X \\ 
 \hline
 Subset & 7.8 & 8.8 \\ 
 \hline
\end{tabular}
\caption{The difference of pre-training on different dataset. The first row show we use the full~\dataset and full UT; The second row show the experiment done with~\dataset subset and UT subset.} 
\label{tab:Synthesis_Realistic_Gaze}
\end{center}
\end{table}


\paragraph{Person Specific Appearance}
Besides head pose and gaze direction ranges, it is well known that person-dependent is always the essayist scenario for gaze estimation, since the similar appearance would make the task much easier. To avoid the tedious personal calibration or long term personal data collection, one option is to use the most similar personal appearance from the training data for testing on target person. However, such training samples selection is limited inside the collected training data. Our method could generate any kind of personal appearance and do not have such limitation. As shown in~\autoref{tab:personal_specific}, there are some subjects that suitable as training set for the MPIIGaze dataset, such as subject 1, 2, 6, 8 and 10. We use these subjects as the training set and can achieve X.X degrees on the MPIIGaze dataset.


\begin{table}
\begin{center}
\begin{tabular}{ |c|c|c|c|c|c| } 
 \hline
 Subject ID & 1 & 2 & 3 & 4& 5 \\ 
 \hline
 Error & 8.5 & 9.0 & 11.65 & 10.72 & 9.3 \\ 
 \hline
 \hline
 Subject ID & 6 & 7 & 8 & 9 & 10 \\ 
 \hline
 Error & 8.3 & 10.4 & 8.5 & 11.1 & 8.6 \\ 
 \hline
\end{tabular}
\caption{The test results on MPIIGaze dataset with the training on individual subjects from~\dataset}
\label{tab:personal_specific}
\end{center}
\end{table}

% \paragraph{Realism and Light Conditions}
% To test the influence of realism and light condition factors, we use the subsets of those datasets that just include frontal face clusters. We first randomly select 10 subjects from UT dataset, and take the frontal pose with their 160 gaze directions as UT frontal subset. Then we generate a~\dataset frontal subset that has the same head pose and gaze distribution with the UT frontal subset in three setting: with varying light conditions, without varying light conditions and without eyelid movement. The test set is the samples in MPIIGaze with head pose within $\pm$ 5 degrees of frontal head pose. The results are shown in~\autoref{tab:frontal_160}.

% \begin{table}
% \begin{center}
% \begin{tabular}{ |c|c| } 
%  \hline
%  Train Set & Estimation Error \\ 
%  \hline
%  SynthEye with varying light condition & 6.8 \\ 
%  \hline
%  SynthEye without varying light condition & 6.7 \\ 
%  \hline
%  SynthEye without eyelid movement & 6.8 \\
%  \hline
% \end{tabular}
% \caption{Result on realism and light conditions}
% \label{tab:frontal_160}
% \end{center}
% \end{table}

% train on synthesised images and show competitive performance on MPIIgaze with real images
% show better performance than UT dataset
%Using Xucong's CNN sytem, we train on targeted version of \dataset, test on MPII. Show results are better than training on UT and testing on MPII. This shows that the range of lighting in \dataset is important for better results.

% does photorealistic data really help/is it necessary? either reduce quality and see how it affects performance, or compare model with and without shape variations
% ^ I am not really sure how to do this well... Because we'd also have to have a measure of "how photorealistic" something is. Swapping the eyeball for a simpler model, e.g. sphere might not really have that much of an effect on "photorealism" for many eye-poses. Changing the shaders, e.g. pretending the skin is Lambertian (diffuse) might?

\commentY{Reminder: ideally we should have a discussion on potential limitations of the current approach}

