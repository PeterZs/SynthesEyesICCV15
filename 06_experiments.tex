%!TEX root = 00_main.tex

\section{Experiments}

%\commentA{the distribution figures would be nice here, e.g. to make the point that we can synthesise arbitrary distributions and to beef up the gaze estimation experiments}

% \begin{figure}
%     \centering
%     \includegraphics[width=\textwidth]{head_gaze_distribution_v2.pdf}
%     \caption{The head pose (first row) and gaze direction (second row) distribution of different datasets.}
%     \label{fig:head_gaze_distribution}
% \end{figure}

\commentA{just a reminder to potentially remove the top row in Figure 8 (head pose distributions)}

% \input{figs/fig_example_fits_wild.tex}

% \commentA{briefly say something about significance/importance of both problems, more in corresponding subsections}

We evaluated the usefulness of our synthetic data generation method on two sample problems, eye-shape registration and appearance-based gaze estimation.
%
Eye-shape registration attempts to detect biological landmarks of the eye -- eyelids, iris and the pupil. 
Such approaches either attempt to model the shape of the eye directly by relying on edge information \cite{wood2014eyetab, swirski2012robust} or by using statistically learnt deformable models \cite{alabort2014statistically}. 
As our method can reliably generate consistent landmark location training data, we use it for Constrained Local Neural Field (CLNF) \cite{baltrusaitis2013constrained} deformable model training.

Appearance-based gaze estimation systems learn a mapping directly from eye image pixels to gaze direction.
% This is in contrast to geometry-based approaches that rely on tracking features of 
This is a challenging task considering the changes in appearance caused by head movement and illumination, thus requiring large amounts of training data to perform well \cite{zhang15_cvpr}.
%
Since our method allows the synthesis of eye images with arbitrary head poses, gaze directions, and illumination conditions, we can generate a suitable training dataset rapidly, and even tailor it to the target domain.


\subsection{Eyelid Registration In the Wild}

\begin{figure}
    \centering
    \includegraphics[width=\columnwidth]{figs/CLNF_300W_experiment.pdf}
    \caption{We outperform the state-of-the-art for eyelid-registration in the wild. The right plot shows how performance degrades for training data without important degrees of variation: realistic lighting and eyelid movement.}
    \label{fig:clnf_results_wild}
\end{figure}

% \input{figs/fig_example_fits_wild.tex}

% Erroll to Tadas: need to make sure these facts are straight...
We performed an experiment to see how our our system generalises on unseen and unconstrained images from the 300 Faces In-the-Wild (300-W) \cite{sagonas2013300} validation datasets which contain labels for eyelid boundaries. We tested all of the approaches on the 830 (out of 1026) test images. We discarded images that did not contain visible eyes (occluded by hair or sunglasses) or where face detection failed in some of the baselines. This lead to 1660 eye images for evaluation.

We trained CLNF patch experts using the generic \dataset dataset and used the 3D landmark locations to construct a Point Distribution Model (PDM) using Principal Component Analysis. 
We initialised our approach by using the face-CLNF \cite{baltrusaitis2013constrained} facial landmark detector.

To compare using synthetic or real training images, we trained an eyelid CLNF model on 300-W images, but used the same PDM used for synthetic data (CLNF 300-W).
We also compared our approach with the following state-of-the-art facial landmark detectors trained on real world in-the-wild data: CLNF \cite{baltrusaitis2013constrained}, Supervised Descent Method \cite{Xiong2013sdm}, Discriminative Response Map Fitting \cite{Asthana2013drmf}, and tree based face and landmark detector \cite{Zhu2012tree}. 

The results of our experiments can be seen in \autoref{fig:clnf_results_wild}. 
Errors were recorded as the RMS point-to-boundary distance from tracked eyelid landmarks to ground truth eyelid boundary, and were normalized by inter-ocular distance. 
First, the results show the eye-CLNF (both synthetic and real data) outperforming all other systems in eye-lid localization. 
Second, our system (CLNF Synth) trained on only ten participants in four lighting conditions results in very similar performance to a system trained on unconstrained in-the-wild images (CLNF 300-W). This suggests the importance of high-quality consistent labels.
% and that our system can be used to achieve state-of-the-art performance for eye shape registration. 
%Furthermore, our approach managed to generalise without explicit modelling of eye-glasses or other partial occlusions.

Our data synthesis system also allows us to examine what steps of the synthesis approach are important for generating good training data. We trained two further eye-CLNFs on different versions of \dataset, one without eyelid motion and one with only one fixed lighting condition. As can be seen in \autoref{fig:clnf_results_wild}, these perform worse as they are missing important degrees of variability.

\subsection{Eye-Shape Registration for Webcams}

% \commentA{``for laptops'' sounds very specific. Does this generalise, do we need this strong constraint?}

\input{figs/fig_clnf_fits.tex}

\begin{figure}
    \centering
    \includegraphics[width=\columnwidth]{CLNF_MPII_experiment}
    \caption{We outperform the state-of-the-art for eyelid-registration in the wild.}
    \label{fig:clnf_results_MPII}
\end{figure}

While the 300-W images represent challenging conditions for eyelid registration, they are not representative of typical webcam-style images and do not feature iris labels.
We therefore annotated sub-pixel eyelid and iris boundaries onto a subset of MPIIGaze (188 images), a recent large-scale dataset of face images and corresponding on-screen gaze locations collected during everyday laptop use over several months~\cite{zhang15_cvpr}.
Pupil accuracy was not evaluated as it was impossible to discern in in most images.
We compared our eye-CLNF with EyeTab \cite{wood2014eyetab}, a state-of-the-art shape-based approach for webcam gaze estimation that robustly fits ellipses to the iris boundary using image-aware RANSAC \cite{swirski2012robust}.
We used the author's implementation, with their suggested modification: improved eyelid localization with a CLNF facial landmark detector \cite{baltrusaitis2013constrained}.
As a baseline, we used the mean position of all 28 eye-landmarks following model initialization.
Eyelid errors were calculated as RMS distances from eyelid landmarks to the ground truth eyelid boudnary.
Iris errors were calculated by first least-squares fitting an ellipse to the tracked iris landmarks, discretizing it, removing points outside ground truth and tracked eyelid boundaries, and then measuring RMS distances to the ground truth iris.
This avoided calculating errors for parts of the iris which were occluded by the eyelid.
Errors were normalized by the eye-width, and are reported using average eye-width ($44.4\textrm{px}$) as reference.

As shown in \autoref{fig:clnf_results_MPII}, our approach ($1.48\textrm{px}$) demonstrates comparable iris-fitting accuracy with EyeTab ($1.44\textrm{px}$), a state-of-the-art specialist algorithm for fitting ellipses to irises in low-quality images.
However, our eye-CLNF is more robust, with EyeTab failing to terminate in $2\%$ of test cases.
As also shown by the 300-W experiment, our eye-CLNF localizes eyelids better than the face-CLNF.
%As shown by \todo{prev exp.}, our eye-CLM ($2.79\!\pm\!2.20\textrm{px}$) also localizes eyelids better than previous state-of-the-art facial landmark trackers ($4.27\!\pm\!1.84\textrm{px}$).

% show that we only need data from few(er) people and show competitive performance
% Maybe) Plot landmark accuracy on LFW against number of training participants. Show that even with just a few participants (e.g. 4) we get good results for eyelid positions compared to state-of-the-art face trackers.

% eye corner detection
% eye bounding box detection
% eye position detection?
% ^ I think all of these come with what the deformable model gives us

\subsection{Appearance-Based Gaze Estimation}

% evaluate eye/gaze/eyelid shapes (fully synthetic) separately from full face appearance (which is a mixture of real and synthetic data)

% person-adaptation, use pre-trained model from synthesised data, then personalise with small amount of user-specific data
% ^ let's leave this as future work! Can put it in the discussion 

% show that we can synthesise specific datasets for specific settings (specific head and gaze ranges, illumination conditions), show that we can competitive performance

\begin{figure}
    \centering
    \includegraphics[width=\columnwidth]{head_gaze_distribution_v2.pdf}
    \caption{The gaze direction (first row, blue) and head pose (second row, red) distributions of different datasets.}
    \label{fig:head_gaze_distribution}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=\columnwidth]{gazeResult.pdf}
    \caption{Gaze estimation performance comparison. The X axis indicates the test set is the MPIIGaze dataset or its frontal pose subset. The legend shows different training sets.}
    \label{fig:gazeResult}
\end{figure}

To evaluate the usage of our method on appearance gaze estimation, we perform the cross-dataset validation as described in Zhang et al.~\cite{zhang15_cvpr}, where they train and test the model on different datasets. Following the same setting, we train the same Convolutional Neural Network (CNN) model on the generic~\dataset dataset, and test it on MPIIGaze evaluation subset proposed by~\cite{zhang15_cvpr}. As shown with the two bars at the far left of~\autoref{fig:gazeResult}, compared to the mean gaze direction prediction error 13.9 degrees that trained with UT dataset~\cite{sugano2014learning}, the model trained with our generic~\dataset dataset can achieve the same performance with 14.0 degrees mean error. It confirms that our method can generate a equivalent data for the appearance-based gaze estimation training.

\paragraph{Head Pose and Gaze Ranges}
The head pose and gaze direction range are also quite important priors for appearance-based gaze estimation. Since the samples from the small range of head poses can be trained together, while extreme head poses wouldn't share eye appearance. Also the gaze direction space is arbitrary and difficult to be densely covered by collected dataset. 
We render a~\dataset subset that matches the gaze and pose distribution of MPIIGaze dataset, with added 3D laptop screen emitting light. Notice that the head pose and gaze direction range can be estimated based on the task, so the required information is not sophisticated. This shows how we can target specific scenarios like laptop-based gaze estimation, and render a suitable dataset within a day rather than taking 3 months of data collection with 15 participants. 
For comparison, we also re-sample a subset of UT dataset as described in~\cite{zhang15_cvpr} that has the same gaze and head pose distribution with MPIIGaze. The head pose and gaze direction distribution are shown in~\autoref{fig:head_gaze_distribution}. Since there are just have 10 subjects in our~\dataset, we divide the UT Multiview subset into 5 groups with 10 subjects. Each group of this UT subset has 15,000 samples as with our~\dataset subset. We then average the performance of the 5 groups for the final result. 
As shown in the third and forth bars of~\autoref{fig:gazeResult}, having the similar head pose and gaze direction ranges of target domain can significantly improve the performance. And also our~\dataset subset is a better training set compared to UT subset. Since both UT subset and~\dataset subset have the similar head pose and gaze direction range, the performance gains should come from the realistic samples generate from precis 3D eye model and also simulated variant appearance caused by light conditions.
The fifth bar of~\autoref{fig:gazeResult} shows the person-independent validation within the MPIIGaze dataset to show the up bounder of this scenario.

\paragraph{Difference Between Synthesis and Realistic Data}
The UT dataset is synthetically generated based on collected data, while our~\dataset is purely synthesis, the different affect between them for the gaze estimation training is interesting. We thus based on the the above experiments to play around their roles in such cross datasets scenario.
We use the pre-trained model on the~\dataset and fine tune it with the UT dataset to test on MPIIGaze dataset, it is the case that we can pre-train the model with the synthetic~\dataset and fine tune the model if we have more realistic data. For caparison, we test the opposite way, i.e. pre-train the model in UT dataset and fine tune it with the~\dataset. As shown in~\autoref{tab:Synthesis_Realistic_Gaze}, our~\dataset is suitable as training data due to its generality while UT doesn't have such quality. 

\begin{table}
\begin{center}
\begin{tabular}{ |c|p{80pt}|p{80pt}| } 
 \hline
 Train set & pre-train~\dataset and fine tune with UT & pre-train UT and fine tune with~\dataset \\ 
 \hline
 Full & X.X & X.X \\ 
 \hline
 Subset & 7.8 & 8.8 \\ 
 \hline
\end{tabular}
\caption{The difference of pre-training on different dataset. The first row show we use the full~\dataset and full UT; The second row show the experiment done with~\dataset subset and UT subset.} 
\label{tab:Synthesis_Realistic_Gaze}
\end{center}
\end{table}


\paragraph{Person Specific Appearance}
Besides head pose and gaze direction ranges, it is well known that person-dependent is always the essayist scenario for gaze estimation, since the similar appearance would make the task much easier. To avoid the tedious personal calibration or long term personal data collection, one option is to use the most similar personal appearance from the training data for testing on target person. However, such training samples selection is limited inside the collected training data. Our method could generate any kind of personal appearance and do not have such limitation. As shown in~\autoref{tab:personal_specific}, there are some subjects that suitable as training set for the MPIIGaze dataset, such as subject 1, 2, 6, 8 and 10. We use these subjects as the training set and can achieve X.X degrees on the MPIIGaze dataset.


\begin{table}
\begin{center}
\begin{tabular}{ |c|c|c|c|c|c| } 
 \hline
 Subject ID & 1 & 2 & 3 & 4& 5 \\ 
 \hline
 Error & 8.5 & 9.0 & 11.65 & 10.72 & 9.3 \\ 
 \hline
 \hline
 Subject ID & 6 & 7 & 8 & 9 & 10 \\ 
 \hline
 Error & 8.3 & 10.4 & 8.5 & 11.1 & 8.6 \\ 
 \hline
\end{tabular}
\caption{The test results on MPIIGaze dataset with the training on individual subjects from~\dataset}
\label{tab:personal_specific}
\end{center}
\end{table}

\paragraph{Realism and Light Conditions}
To test the influence of realism and light condition factors, we use the subsets of those datasets that just include frontal face clusters. We first randomly select 10 subjects from UT dataset, and take the frontal pose with their 160 gaze directions as UT frontal subset. Then we generate a~\dataset frontal subset that has the same head pose and gaze distribution with the UT frontal subset in three setting: with varying light conditions, without varying light conditions and without eyelid movement. The test set is the samples in MPIIGaze with head pose within $\pm$ 5 degrees of frontal head pose. The results are shown in~\autoref{tab:frontal_160}.

\begin{table}
\begin{center}
\begin{tabular}{ |c|c| } 
 \hline
 Train Set & Estimation Error \\ 
 \hline
 SynthEye with varying light condition & 6.8 \\ 
 \hline
 SynthEye without varying light condition & 6.7 \\ 
 \hline
 SynthEye without eyelid movement & 6.8 \\
 \hline
\end{tabular}
\caption{Result on realism and light conditions}
\label{tab:frontal_160}
\end{center}
\end{table}

% train on synthesised images and show competitive performance on MPIIgaze with real images
% show better performance than UT dataset
%Using Xucong's CNN sytem, we train on targeted version of \dataset, test on MPII. Show results are better than training on UT and testing on MPII. This shows that the range of lighting in \dataset is important for better results.

% does photorealistic data really help/is it necessary? either reduce quality and see how it affects performance, or compare model with and without shape variations
% ^ I am not really sure how to do this well... Because we'd also have to have a measure of "how photorealistic" something is. Swapping the eyeball for a simpler model, e.g. sphere might not really have that much of an effect on "photorealism" for many eye-poses. Changing the shaders, e.g. pretending the skin is Lambertian (diffuse) might?

